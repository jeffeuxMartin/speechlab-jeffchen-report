
\section{實驗集與分析模型}

%%%%%%%%%
%%%%%%%%%% %%%%%%%%%%%%%%%%
%%%%%%%%%% 本研究的分析對象參考無文字架構 \cite{noauthor_textless_2021, lakhotia_generative_2021, lakhotia_generative_2021-1} 的研究，採用當中提及的 CPC、Wav2vec 2.0 和 HuBERT 三個語音基石模型，並與作為比對的聲學特徵梅爾時頻譜（Mel-Spectrogram），共四種語音表徵模型。模型的詳細資訊如表 \ref{tab:model-info} 所示：
%%%%%%%%%
%%%%%%%%%% \begin{table}
%%%%%%%%%%     \centering
%%%%%%%%%%     \begin{tabular}{|c|l|c|c|} \hline 
%%%%%%%%%%          語音表徵&   模型架構&離散表徵來源層數& 時間解析度（Time Resolution）\\ \hline 
%%%%%%%%%%          HuBERT      &   CNN + Transformer&6& 20 毫秒\\ \hline 
%%%%%%%%%%          Wav2vec 2.0 &   CNN + Transformer&14& 20 毫秒\\ \hline 
%%%%%%%%%%          CPC         &   CNN + GRU&2& 10 毫秒\\ \hline 
%%%%%%%%%%          LogMel      &   聲學特徵&N/A& 10 毫秒\\ \hline
%%%%%%%%%%     \end{tabular}
%%%%%%%%%%     \caption{語音離散表徵的來源層數與音框時間解析度}
%%%%%%%%%%     \label{tab:model-info}
%%%%%%%%%% \end{table}
%%%%%%%%%
%%%%%%%%%% 以下是各語音表徵模型的簡介：
%%%%%%%%%
%%%%%%%%%% \begin{itemize}
%%%%%%%%%%     \item **CPC（對比預測編碼）**：CPC 由編碼器和預測器兩部分組成。編碼器從語音輸入生成嵌入 z。預測器基於過去預測編碼器的未來狀態，系統通過對比損失進行訓練。我們使用 Rivière 和 Dupoux（2020）訓練的 CPC 模型，該模型使用 LibriLight 數據集的6k小時“乾淨”子樣本進行訓練。我們從預測器的中間層提取表示，生成256維的嵌入（每10毫秒一個），如原始論文所述。
%%%%%%%%%%     \item **Wav2vec 2.0**：此模型使用編碼器和預測器，並通過對比性地區分編碼器輸出中的正樣本和負樣本進行訓練。我們使用 Baevski 等人（2020b）訓練的預訓練 Wav2vec 2.0 LARGE 變體，該模型在 LibriLight 數據集的60k小時上進行訓練。此模型將原始音頻編碼為1024維的向量幀（每20毫秒一個）。為了選擇最佳層，我們從模型的每一層提取10小時的 LibriLight 子集的凍結表示，並使用 CTC 損失訓練線性分類器來預測文本標籤的音素版本。第14層在 LS dev-other 上獲得了最低的 PER（Baevski 等人，2021年使用類似的方法選擇了第15層）。
%%%%%%%%%%     \item **HuBERT**：與 CPC 和 Wav2vec 2.0 使用對比損失不同，HuBERT 使用類似於 BERT（Devlin 等人，2019）的掩碼預測任務進行訓練，但輸入是掩碼連續音頻信號。目標是通過對原始語音特徵或早期迭代的學習特徵進行無監督聚類獲得的，這一方法受 DeepCluster（Caron 等人，2018）的啟發。我們使用了 Hsu 等人（2021）訓練的 BASE 12 層變壓器模型，該模型在960小時的 LibriSpeech 上進行了兩次迭代訓練。此模型在每層將原始音頻編碼為768維的向量幀（每20毫秒一個），我們從第6層提取這些向量，如原始論文所述。
%%%%%%%%%%     \item **LogMel**：作為基準，我們考慮使用80個頻帶的對數梅爾濾波銀行編碼器。
%%%%%%%%%% \end{itemize}
%%%%%%%%%% %================


%%%%%%%%%%%%%%%%
　　
本研究的分析對象參考無文字架構 \cite{noauthor_textless_2021, lakhotia_generative_2021, lakhotia_generative_2021-1} 的研究，
% 採用當中提及的 CPC、Wav2vec 2.0 和 HuBERT 三個語音基石模型，並與作為比對的聲學特徵梅爾時頻譜（Mel-Spectrogram），共四種語音表徵模型。
% 模型的
% 資訊簡述如下：
採用論文中提及的四種語音表徵，簡述如下：

\begin{itemize}
    \item CPC：卷積式編碼器 + 遞迴式預測器，以對比式學習訓練。表徵來自預測器的中間層，每 10 毫秒提取一個向量表徵作為音框
    \item Wav2vec 2.0：卷積式編碼器 + 轉換器預測器，以對比式學習訓練。表徵來自轉換器第 14 層，每 20 毫秒作為一個音框
    \item HuBERT：卷積式編碼器 + 轉換器預測器，以預測式學習訓練，其訓練目標為 K-平均分群演算法的結果，透過遮蔽語言模型的方式訓練。表徵來自轉換器第 6 層，每 20 毫秒作為一個音框
    \item LogMel：為 80 維對數梅爾時頻譜的聲學特徵，在此作為比較基線（Baseline）。音框寬度為 10 毫秒

          % CPC 由編碼器和預測器兩部分組成。編碼器從語音輸入生成嵌入 z。預測器基於過去預測編碼器的未來狀態，系統通過對比損失進行訓練。我們使用 Rivière 和 Dupoux（2020）訓練的 CPC 模型，該模型使用 LibriLight 數據集的6k小時“乾淨”子樣本進行訓練。我們從預測器的中間層提取表示，生成256維的嵌入（每10毫秒一個），如原始論文所述。
          % \item Wav2vec 2.0：此模型使用編碼器和預測器，並通過對比性地區分編碼器輸出中的正樣本和負樣本進行訓練。我們使用 Baevski 等人（2020b）訓練的預訓練 Wav2vec 2.0 LARGE 變體，該模型在 LibriLight 數據集的60k小時上進行訓練。此模型將原始音頻編碼為1024維的向量幀（每20毫秒一個）。為了選擇最佳層，我們從模型的每一層提取10小時的 LibriLight 子集的凍結表示，並使用 CTC 損失訓練線性分類器來預測文本標籤的音素版本。第14層在 LS dev-other 上獲得了最低的 PER（Baevski 等人，2021年使用類似的方法選擇了第15層）。
          % \item HuBERT：與 CPC 和 Wav2vec 2.0 使用對比損失不同，HuBERT 使用類似於 BERT（Devlin 等人，2019）的掩碼預測任務進行訓練，但輸入是掩碼連續音頻信號。目標是通過對原始語音特徵或早期迭代的學習特徵進行無監督聚類獲得的，這一方法受 DeepCluster（Caron 等人，2018）的啟發。我們使用了 Hsu 等人（2021）訓練的 BASE 12 層變壓器模型，該模型在960小時的 LibriSpeech 上進行了兩次迭代訓練。此模型在每層將原始音頻編碼為768維的向量幀（每20毫秒一個），我們從第6層提取這些向量，如原始論文所述。
          % \item LogMel：作為基準，我們考慮使用80個頻帶的對數梅爾濾波銀行編碼器。
\end{itemize}


% \ref{tab:model-info} 所示：

% \begin{table}
%     \centering
%     \begin{tabular}{|c|l|c|c|} \hline 
%          語音表徵&   模型架構&離散表徵來源層數& 時間解析度（Time Resolution）\\ \hline 
%          HuBERT      &   CNN + Transformer&6& 20 毫秒\\ \hline 
%          Wav2vec 2.0 &   CNN + Transformer&14& 20 毫秒\\ \hline 
%          CPC         &   CNN + GRU&2& 10 毫秒\\ \hline 
%          LogMel      &   聲學特徵&N/A& 10 毫秒\\ \hline
%     \end{tabular}
%     \caption{語音離散表徵的來源層數與音框時間解析度}
%     \label{tab:model-info}
% \end{table}
我們使用該論文釋出之預訓練模型與 K-平均量化模型。預訓練模型細節詳述於 \cite{lakhotia_generative_2021-1} 中，量化模型則是該篇論文透過公開的 LibriSpeech 資料集 \cite{panayotov_librispeech_2015} 中 train-clean-100 訓練集
，獲取語音表徵後執行 K-平均分群演算法所得，並釋出群數為 50、100 和 200 的三個版本。

% 此後亦跟隨該研究選用特定模型層數及其釋出之 K-平均量化模型。這些模型層數與量化模型在該研究中被證明與語音學特徵最相關，且被使用於無文字架構後續研究之語音離散表徵的抽取方法。無文字研究中 \cite{lakhotia_generative_2021-1} 已透過公開的 LibriSpeech 資料集之 train-clean-100 訓練集
% 語料
% 對四種語音表徵進行 K-平均分群演算法，分別得到群數為 50、100 和 200 的三個量化模型。

% 本論文以公開的 LibriSpeech 資料集為分析對象，採取其 train-clean-100 為分析的語音語料庫。
本論文亦以 LibriSpeech train-clean-100 作為分析對象，
% 因此，
% 本研究
將語音語料庫的語音資料經過四個模型獲取連續表徵後，再經過量化模型得到完全由離散單元組成的「偽文字」語料。

針對語音學的音位標註，透過強迫對齊器（Forced-aligner）\footnote{https://github.com/MontrealCorpusTools/Montreal­Forced­Aligner}的英語預訓練模型，從語料庫的文字轉寫取得語音資料的音位標註與對應的時間範圍。最後透過語音表徵各自的時間解析度生成以音框為單位的音位標註語料。最後將兩者對語音資料集進行音位標註相關性的分析。
%================
