
\textcolor{gray}{stat 只要詞表就好（piece ＆ freq）}

\textcolor{gray}{mel 有緣再說管他去死，先 hubert}
\textcolor{gray}{hubert 先 50 跟 100 就好，200 先不管}
\textcolor{gray}{ap  先 (100?) 500 跟 1000 就好，後面先不管}

\mychcnt{4}
\chapter{多個單元} \cite{-}

\section{BPE intro} 
        ......

\section{How--Basic ＆ Real}
　　為了確認，我們先 fix 在 HuBERT 50 跟 100 為基礎，觀察原來、500、1000 AcPcs 的差異。

\section{分析結果}

\subsection{綜觀角度 + 以離散單元？要怎麼切？}
        於是，我們承繼上一章的作法，我們先比較不同的聲學片段數量對於機率熱圖與純度數據的影響。從圖中可以看出，當聲學片段數量上升時，機率熱圖整體雖然變得更碎片化，但機率高、集中的格子變多了。\textcolor{red}{［注意一下你可能是在講 $Pr_u(\varphi)$ 而不是第三章的 $Pr(u, \varphi)$ ] }
        %%% =================
        關於這件事，我們可以觀察音位熵直方圖的變化。整體我們可以看到，當今天使用分詞方法之後，整體音位熵確實是有所降低的，也就是各自符記所對應的音位變得更加集中而明確了。不過缺點當然是加入了太多符記，因此歸類同樣音位的效果也變得很不明顯。
        而且，接續前面的觀察，符記數量上升時，確實可以讓聲學片段捕捉到更加細節的語音資訊。至少從以分群數為 50 的單位採取分詞方法處理，以 500、1000 次詞單位進行處理後，我們發現原本在分群數為 50，也就是整體有 50 種符記的情形，塞擦音是沒有辦法被單獨符記代表的，而進行次詞單位的分詞後，塞擦音就可以被適當編碼了。
        %%% --------------------------------
        不過，比起聲學片段的詞表大小，我們發現整體模型對應音位的好壞還是直接取決於 K-平均分群的數量，即便是使用分詞方法設定詞表大小為 100 的分詞處理在分群數為 50 的模型之上，其效果仍然略差於一開始分群數就是 100 的離散單元表現。\textcolor{red}{［畢竟有可能是在表徵空間做的一定更好。] }然而，光是就整體表現有提升這點，在無法使用大量 K-平均分群數時，採取分詞方法仍然是有幫助的。
        %%%
        接下來，我們也可以像上一章節那樣，觀察聲學片段的混淆關係，也就是前幾高音位之間的關係。然而，因為都已經有這麼多符記了，因此\textcolor{magenta}{［確認一下機率齁！尤其是離散單元的音位熵] } \textcolor{red}{［沒救，看來只是因為符記太多了的關係，並不是因為機率的關係] }既然第一高的音位都已經機率高很多，也就是整個條件音位分佈已經變得這麼集中，那後面的其他音位相對關係就變得薄弱了。其中一個緣由也可能是因為，聲學片段本身可以把帶有不同代表性音位的離散單元合在一起，\textcolor{red}{［這邊後面有緣用 triunits 去處理] } 因此整體對特定音位的代表性跟語音相關性就低上了不少。\textcolor{gray}{〔plosive 降低而 fricative 上升這點先藏招不要講好了……〕 }
        再來，我們也可以從「用音位分類為新標籤計算的純度」數據來證明這件事。這裡可以看到，隨著詞表大小的上升，整體的純度只有些微提升，而且愈來愈不明顯。由此可見上述的「後面的其他音位相對關係就變得薄弱了」這點確實存在，整體可能已經沒什麼提升空間了。
        (((( again for based units )))) --> 先藏招好了

\subsection{以音位角度切入}

        然後，我們轉而再度去以音位的角度切入，觀察各自音位的符記分佈集中程度。從這邊，我們可以看到整體排名趨勢與第三章不做分詞時的排名幾乎差不多，足以見得或許音位本身的容易或難以歸類這點，在對語音表徵進行分群時就已經做得到了，而加入分詞方法雖然不會有明顯提升，但明明分詞方法可以把不同代表性音位的離散單元重新合在一起，卻也仍然大致維持了這個趨勢。\textcolor{red}{［可能確認一下 Mel 那邊有沒有救？希望電腦不要死掉……] }\textcolor{gray}{〔至於 S 下降這個個案先不要提……〕 }
        最後，我們可以統計一下各音位分類的純度與相互資訊數據，與上一章節比對。從這邊看了，除了原本純度較低的塞音比較有所提升以外，其他純度較高的音位分類純度提升得很有限，但整體大致仍然是有所提升的。\textcolor{red}{［驗證一下是不是在基底分群數就定生死了，比較 50 跟 100……] }

\subsection{最後觀察整體熱圖}

        我們這邊可以比較一下 50、100、50+100、50+500、100+500 的熱圖。從這邊可以驗證前面所說的：在基底分群數確定的情形之下，的確一開始分群數開得比較大，整體對語音規律的捕捉效果就會比較好，但藉由分群方法的引入，至少 50 分群數可以藉由符記數提升的機會，來重新編碼語音中的結構，也就是雖然不如 K-平均演算法那樣因為直接做用於語音表徵空間，那麼好區分出發音的差異，但藉由分詞演算法，仍然可以從捕捉語音中明顯重複的序列資訊\textcolor{red}{［這時候是不是要擺一下長度 ＆ 常見 pattern 統計數據確認了？] } ，獲取語音序列中跟發音有關的特徵，進而模擬類似人類理解音位的過程。

\begin{itemize}
    \item \textcolor{green}{〈先把上面缺的補上〉 }
    \item \textcolor{green}{〈順語句跟切分節〉 }
    \item \textcolor{green}{《補上 stats 嗎？》 }
    \item \textcolor{green}{《補上 triunits 嗎？》 }
    \item \textcolor{green}{〈最後是重新調整那個「分詞方法」的翻譯跟講法，用「次詞單元」〉 }
\end{itemize}

\section{Conclusion}

        Not bad, but OK.
