
\chapter{多個語音離散單元}   \input{chapters_stable/tinys}

    {  \jeffcomment{bef analysis} 
{  \jeffcomment{pre}   
\section{動機}            　　本章嘗試分詞法。
\section{相關研究} 　　像 Wav2seq \cite{wu_wav2seq_2023}  等是相關研究。
\section{分詞方法} 　　這邊講述 BPE、Unigram 等演算法具體怎麼實行。}
\section{衡量方式}
　　本章節沿用上一章節 LibriSpeech 資料集的 train-clean-100 訓練子集，以及相同的分析數據以進行比對。由於與上一章節的差異僅在分詞方法的引入，因此數據結果將多紀錄分詞前後序列長度的變化；其他操控變因包含分詞方法與詞表大小。考慮到語音訊號本身不如英語等文字，在書寫時就已經具備空格分隔單詞，因此以下分析結果皆採用 SentencePiece 套件中實作之單一詞演算法為分詞方法，並比較詞表大小 500、1000、8000、10000、20000 五種設定的結果差異。（空間不足時則僅呈現 500、1000、10000 三種設定的趨勢。）  }  % 前面的內容
\section{分析結果} 
{
　　以下數據中的「長度壓縮比率」係指透過分詞方法後，每一個句子的「單詞」數與原先離散單元的數量相比之比值。由於長度壓縮比率是針對離散單元分詞所得到，與標註無關，因此只在音位分析的表格上呈現。 }
\subsection{基於各自音位的分析}
　　首先，為了比較不同詞表大小對於純度、相互資訊等數據的影響，先分別固定語音模型為 HuBERT 和 Wav2vec 2.0，在離散單元的分群數量為 50、100、200 三種設定下，觀察詞表大小造成的變化。由 HuBERT （表 \ref{tab:hubert-phn-results}）和 Wav2vec 2.0（表 \ref{tab:w2v2-phn-results}）的數據比較可觀察到，詞表大小上升除了使得音位純度提高以外，相互資訊也是隨之提高的，可以發現使用分詞方法並給予足夠大的詞表，對於找出語音中的資訊確實有所幫助。  \input{tables/chapter4/tab1}  \par %⤵️
        接著從另一個角度切入，比較同樣都是離散單元分群數為 200 的條件下，不同語音基石模型的分析數據。由表 \ref{tab:ch4-models-phn} 可以發現，HuBERT 模型在音位純度與相互資訊勝過其他模型，這個結論與上一章節是一致的。 \par % ⤵️
        有趣的是，觀察長度壓縮比率可以發現，CPC 模型在分詞演算法的引入後，能夠使序列變得最短，但同時在音位純度與相互資訊上也有所犧牲；而 HuBERT 雖然在這些分析數據上高過其他三者，卻同時達成了比 Wav2vec 2.0 和 LogMel 更好的壓縮比率。因此綜合看來，這很可能是目前使用語音離散單元進行研究時，HuBERT 模型仍然是領域內首選的緣由。

\subsection{基於語音學分類的分析}
　　最後觀察將語音標註換成語音學分類的結果，一樣可以從表 \ref{tab:hubert-pcls-results}、\ref{tab:w2v2-pcls-results} 和 \ref{tab:ch4-models-pcls} 觀察到與上一小節相同的趨勢。   \input{tables/chapter4/tab2}

\section{本章總結} {　  藉由引入分詞演算法我們觀察了用分詞法的效果差異。
}
